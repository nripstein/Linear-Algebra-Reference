
\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\makeatletter

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\title{MATH 2LA3 (Applications of Linear Algebra): Course Summary Sheet}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\advance\paperheight1.5in % here is new https://tex.stackexchange.com/questions/649149/cheat-sheet-using-mini-pages-and-tikzpicture-does-not-overflow-properly

\makeatletter  % for augmanted matrix from https://tex.stackexchange.com/questions/2233/whats-the-best-way-make-an-augmented-coefficient-matrix
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% these two packages needed for dotted vertical line in augmented matrix
\usepackage{amsmath}
\usepackage{arydshln} 

\usepackage{multirow} % for multi column

\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

% \begin{center}{\huge{\textbf{MATH 2LA3 Course Summary Sheet}}}\\ % CAN GET RID OT THIS LINE AND ONE BELOW TO CRAM MORE SPACE
% \end{center}
\begin{multicols*}{3}

\tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

%------------ Orthogonal Matrices ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $Q_{n \times n}$ is an orthogonal matrix iff has orthonormal cols:\\
    Properties:\\
        \indent\hspace{0.5cm}1. $(Q\vec{x})^T(Q\Vec{z}) = \vec{x}^Tz$ (angle preserving)\\
        \indent\hspace{0.5cm}2. $||Q\vec{x}||=||\vec{x}||$ (length preserving)\\
        \indent\hspace{0.5cm}3. $Q^T = Q^{-1}$ (true iff $Q$ is orthogonal)\\
        \indent\hspace{0.5cm}4. $\det(Q) = \pm 1$\\
        \indent\hspace{0.5cm}5. Product of orthogonal matrices is orthogonal\\
    $\det(Q) = 1$ for rotation $\det(Q) = -1$ for reflection
    \end{minipage}
};
%------------ Orthogonal Matrices Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Matrices};
\end{tikzpicture}

%------------ Diagonalization Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $D = 
    \begin{bmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \ddots & 0 \\
        0 & 0 & \lambda_n
    \end{bmatrix}$, 
    $P = \begin{bmatrix}
        \vec{\text{eigen}_{\lambda_1}} & \hdots & \vec{\text{eigen}_{\lambda_n}}
    \end{bmatrix}$\\
    $A$ and $D$ are similar $\Leftrightarrow A = PDP^{-1}$.\\
    $\bullet A$ is similar to itself.\\
    $\bullet A$ diagonalizable $\Leftrightarrow A^n = PD^nP^{-1}$\\
    \textbf{$A$ is diagonalizable iff} (equivalent statements):\\
        $\bullet$ $A$ has $n$ linearly independent eigenvectors\\
        $\bullet$ $\sum_{i} \text{GM}(\lambda_i) = n$. GM($\lambda$) := number of eigenvectors for eigenvalue $\lambda$
    \end{minipage}
};
%------------ Diagonalization Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Diagonalization: $A = PDP^{-1}$};
\end{tikzpicture}

%------------ Dot Product Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $\bullet \begin{bmatrix}
        u_1 \\
        u_2
    \end{bmatrix} \cdot
    \begin{bmatrix}
        v_1\\
        v_2
    \end{bmatrix} = (u_1)(v_1) + (u_2)(v_2)
    $\\
    $\bullet \vec{u} \cdot \vec{v} = \vec{u}^T \vec{v}$\\
    $\bullet \vec{u} \cdot \Vec{u} = \left( ||\vec{u}|| \right)^2$\\
    $\bullet \vec{u} \cdot \vec{v} = 0 \Leftrightarrow \vec{u}$, $\vec{v}$ orthogonal $\Leftrightarrow ||\vec{u}|| + ||\vec{v}|| = ||\vec{u} + \vec{v}||$\\
    $\bullet (\vec{a} + \vec{b}) \cdot (\vec{c} + \Vec{d}) = \vec{a} \cdot \vec{c} + \vec{a} \cdot \vec{d} + \vec{b} \cdot \vec{c} + \vec{b} \cdot \vec{d}$\\
    $\bullet ||\begin{bmatrix} v_1 & v_2 \end{bmatrix}^T|| = \sqrt{v_1^2 + v_2^2}$
    \end{minipage}
};
%------------ Dot Product Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Dot Product};
\end{tikzpicture}

%------------ Eigenvalues and Eigenvectors Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    Eigenvalues of triangular matrix along the diagonal.\\
    $\bullet$ AM($\lambda = 0$) = nullity($A$)\\
    To \textbf{Find Eigenvalues} of $A = 
    \begin{bmatrix}
        -5 & 2 \\
        -9 & 6
    \end{bmatrix}$\\
    Solve $ |A-\lambda I_n| = 0 \Leftrightarrow
        \begin{bmatrix}
            -5 - \lambda & 2 \\
            -9 & 6 - \lambda
        \end{bmatrix} = 0$

    To \textbf{Find Eigenvectors for $\lambda_i$}, solve:\\
        $\begin{bmatrix}[cc:c]
            -5 - \lambda_i & 2 & 0 \\
            -9 & 6 - \lambda_i & 0
        \end{bmatrix}$
    \end{minipage}
};
%------------ Eigenvalues and Eigenvectors Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Eigenvalues and Eigenvectors};
\end{tikzpicture}

%------------ Linear Programming Content---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \textbf{Canonical formulation:} Maximize $f(\vec{x}) = \vec{c}^T\vec{x}$\\
            \indent\hspace{0.5cm} Subject to $A\vec{x} \leq \vec{b}; \vec{x_i} \geq 0 \forall i$\\
        \textbf{Solution:} Feasible intersection point which maximizes objective function
    \end{minipage}
};
%------------ Linear Programming Header---------------------
\node[fancytitle, right=10pt] at (box.north west) {Linear Programming};
\end{tikzpicture}

%------------ Orthogonal Compliments ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    Definition: if $W$ is a subspace of $\mathbb{R}^n$, then $W^{\perp}$ contains all vectors $\in \mathbb{R}^n$ perpendicular to $W$\\
    \textbf{Properties:}\\
        \indent\hspace{0.5cm}1. $W^{\perp} =$ Null($A^T$)\\
        \indent\hspace{0.5cm}2. dim($W$) + dim($W^{\perp}) = n$\\
        \indent\hspace{0.5cm}3. $(W^{\perp})^{\perp} = W$\\
        \indent\hspace{0.5cm}4. $W \cap W{^\perp} = \{ \vec{0} \}$\\
        \indent\hspace{0.5cm}5. $(\mathbb{R}^n)^{\perp} = \{ \vec{0} \}$; $\{ \vec{0} \}^{\perp} = \mathbb{R}^n$\\
    \textbf{Subspaces of $\mathbb{R}^n$:}\\
        \indent\hspace{0.5cm}$\bullet$ Row($A$) = (Nul($A$))$^{\perp}$\\
        \indent\hspace{0.5cm}$\bullet$ Nul($A$) = (Row($A$))$^{\perp}$\\
        \indent\hspace{0.5cm}$\bullet$ dim(Row($A$)) = Rank($A$)\\
        \indent\hspace{0.5cm}$\bullet $dim(Nul($A$)) = $n$ - Rank($A$)\\
    \textbf{Subspaces of $\mathbb{R}^m$:}\\
        \indent\hspace{0.5cm}$\bullet$ Col($A$) = (Nul($A^T$))$^{\perp}$\\
        \indent\hspace{0.5cm}$\bullet $Nul($A^T$) = (Col($A$))$^{\perp}$\\
        \indent\hspace{0.5cm}$\bullet $dim(Col($A$)) = Rank($A$)\\
        \indent\hspace{0.5cm}$\bullet $dim(Nul($A^T$)) = $m$ - Rank($A$)\\
    \textbf{Example: find a basis for Row($A$)}\\
    $\vec{v} = \begin{bmatrix} 1 & 2 & 1 & 2 \end{bmatrix}^T$, and $A\vec{v} = \vec{0}$\\
    $A\vec{v} = \vec{0}$, so $\vec{v} \in$ Nul($A$).\\
    Let $W = $Nul($A$), so $W^\perp =$ Row($A$)\\
    $\Rightarrow $ Nul($\Vec{v}^T$) is a basis for Row($A$).
    \begin{align*}
        \text{Nul}(\begin{bmatrix} 1&2&1&2 \end{bmatrix}) &= \left\{ \begin{bmatrix} -2\\ 1\\ 0\\ 0 \end{bmatrix}, \begin{bmatrix} -1\\ 0\\ 1\\ 0 \end{bmatrix}, \begin{bmatrix} -2\\ 0\\ 0\\ 1 \end{bmatrix} \right\} \\
        &= \text{basis for Row}(A)
    \end{align*}
        
    \end{minipage}
};
%------------ Orthogonal Compliments Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Compliments};
\end{tikzpicture}



%------------ Span Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    \textbf{Def} The span of a set of vectors is the set of linear combinations of the vectors.\\
    Check if $\vec{w} \in $span$\{\vec{u}, \vec{v} \}$:\\
    $\begin{bmatrix}[cc:c]
            u_1 & v_1 & w_1 \\
            u_2 & v_2 & w_2
        \end{bmatrix}$ $\vec{w} \in $span$\{\vec{u}, \vec{v} \}$ iff that system has a solution.

    Check if span$\{\vec{u}, \vec{v}, \vec{w}\} = \mathbb{R}^3:$\\ (equivalent to col[$\Vec{u}, \vec{v}, \vec{w}$] = $\mathbb{R}^3$)\\
    $\begin{bmatrix}
            u_1 & v_1 & w_1 \\
            u_2 & v_2 & w_2 \\
            u_3 & v_3 & w_3
        \end{bmatrix}$ True iff $\exists$ a pivot in each row/column
    \end{minipage}
};
%------------ Span Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Span};
\end{tikzpicture}

%------------ Orthogonal Decomposition Content---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $W$ a subspace of $\mathbb{R}^n \Leftrightarrow y = \hat{y} + z$, s.t. $\hat{y} \in W, z \in W^\perp$\\
        $\hat{y} = proj_{u_1}y +...+ proj_{u_p}y = proj_{W}y = \sum_{i=1}^p \frac{y \cdot u_i}{||u_i||^2}$
    \end{minipage}
};
%------------ Orthogonal Decomposition Header---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Decomposition};
\end{tikzpicture}

%------------ Matrix Properties Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $A_{m \times n} \Rightarrow $ transformation $\vec{x} \rightarrow A\vec{x}$ is from $\mathbb{R}^n \rightarrow \mathbb{R}^m$\\
    Rank$(A)$ + nullity($A$) = $n$\\
    $A^TA$ \& $AA^T$:\\
    \indent\hspace{0.5cm}1. have same non 0 eigenvalues\\
    \indent\hspace{0.5cm}2. Symmetric, positive semidefinite\\
    $\bullet A^TA$ invertible iff $A$ has independent columns
        \hrule
    
    \small{
    	\begin{tabular}{lp{4cm} l}
		\textbf{Determinants} & $\det(AB) = \det(A)\det(B)$ \\
                & $\det(A^{-1}) = \frac{1}{\det(A)}$ \\
                & $\det(\text{adj}(A)) = \det(A)^{n-1}$ \\
                & $\det(A^n) = ({\det(A)})^n$ \\
                & $\det(cA) = c^n\det(A)$ \\ 
                & $\det(A^T) = \det(A) = |A|$ \\ 
                & $\det(A_{\text{tringl}}) = \prod_{i=1}^n A_{(i,i)}$ \\ 
                & $\det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc$\\
            \hline

		\textbf{Inverse} & $(AB)^{-1} = B^{-1}A^{-1}$ \\
                & $(A^{-1})^{-1} = A$ \\
                & $(kA^{-1}) = \frac{1}{k}A$ \\
                & $(A^{-1})^{-1} = I_n$ \\ 
                
                & \multirow{2}{*}{$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$} \\
                & \\
                & \\
            \hline
            
		\textbf{Transpose} & $(A^T)^T = A$ \\
                & $(AB)^{T} = B^{T}A^{T}$ \\
                & $(kA^{T}) = kA^T$ \\
                & $(A + B)^T = A^T + B^T$ 
            % \hline
	\end{tabular}}
    \end{minipage}
};
%------------ Matrix Properties Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Matrix Properties};
\end{tikzpicture}


%------------ Injectivity and Surjectivity Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \textbf{Injective:} (one-to-one)\\
            \indent\hspace{0.5cm}1. $f(x) = f(y) \leftarrow x = y$ (map to the same point)\\
            \indent\hspace{0.5cm}2. rank($A$) = n\\
            \indent\hspace{0.5cm}3. pivot in every column\\
        \textbf{Surjective:} (onto)\\
            \indent\hspace{0.5cm}1. $\forall \vec{b} \in \mathbb{R}^m, \exists \vec{x} \in \mathbb{R}^n$ such that $f(\vec{x}) = \vec{b}$\\
            \indent\hspace{0.5cm}2. rank($A$) = m\\
            \indent\hspace{0.5cm}3. pivot in every row 
    \end{minipage}
};
%------------ Injectivity and Surjectivity Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Injectivity and Surjectivity};
\end{tikzpicture}

%------------ Gram-Schmidt Process Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \textbf{Gram-Schmidt Process:} (orthogonalize basis)\\
        $\vec{v_1} = \vec{x_1}$ (remember to normalize at end)\\
        $\vec{v_2} = \vec{x_2} - \frac{\vec{x_2} \cdot \vec{v_1}}{\vec{v_1} \cdot \vec{v_1}}\vec{v_1}$\\
        $\vec{v_p} = \vec{x_p} - \frac{\vec{x_p} \cdot \vec{v_1}}{\vec{v_1} \cdot \vec{v_1}}\vec{v_1} - \frac{\vec{x_p} \cdot \vec{v_2}}{\vec{v_2} \cdot \vec{v_2}}\vec{v_2}- ... \frac{\vec{x_p} \cdot \vec{v_{p-1}}}{\vec{v_{p-1}} \cdot \vec{v_{p-1}}}\vec{v_{p-1}}$\\
        \textbf{QR Factorization ($A = QR$):}\\
        $A_{n\times q}$ has independent columns.  Apply Gram-Schmidt on columns to get $\{\vec{v_1},..,\vec{v_q} \}$, then normalize to $\{\vec{u_1},..,\vec{u_q} \}$\\
        $\bullet Q = [\vec{u_1},..,\vec{u_q}]$\\
        $\bullet R = Q^T A$
    \end{minipage}
};
%------------ Gram-Schmidt Process Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Gram-Schmidt Process};
\end{tikzpicture}

%------------ Invertible Matrix Theorem Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        1. A is row-equivalent to the \(n \times n\) \(I_n\). \\
        2. A has \(n\) pivot positions. \\
        3. The equation \(Ax=0\) has only the trivial solution. \\
        4. The columns of A form a linearly independent set. \\
        5. The linear transformation \(x \mapsto Ax\) is one-to-one. \\
        6. $\forall \vec{b} \in \mathbb{R}^n$, $A\vec{x}=\vec{b}$ has a unique solution. \\
        7. The columns of A span \(\mathbb{R}^n\). \\
        8. The linear transformation \(x \mapsto Ax\) is a surjection. \\
        9. There is an \(n \times n\) matrix \(C\) such that \(CA=I_n\). \\
        10. There is an \(n \times n\) matrix \(D\) such that \(AD=I_n\). \\
        11. The transpose matrix \(A^T\) is invertible. \\
        12. The columns of A form a basis for \(\mathbb{R}^n\). \\
        13. The column space of A is equal to \(\mathbb{R}^n\). \\
        14. The dimension of the column space of A is \(n\). \\
        15. The rank of A is \(n\). \\
        16. The null space of A is \(\{0\}\). \\
        17. The dimension of the null space of A is 0. \\
        18. 0 fails to be an eigenvalue of A. \\
        19. det($A) \neq 0$  \\
        20. The orthogonal complement of Col($A$) is \(\{0\}\). \\
        21. The orthogonal complement of Nul($A$) is \(\mathbb{R}^n\). \\
        22. The row space of A is \(\mathbb{R}^n\).
        
    \end{minipage}
};
%------------ Invertible Matrix Theorem Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Invertible Matrix Theorem};
\end{tikzpicture}


%------------ Projections Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \indent\hspace{0.5cm}1. Projection matrices are symmetric.\\
        \indent\hspace{0.5cm}2. $P^2 = P$ ($1 \& 2 \Leftrightarrow P$  is projection matrix) \\
        $\bullet P$ projects onto Col($P$)\\
        $\bullet proj_{\vec{u}}\Vec{v} = \frac{\vec{v} \cdot \vec{u}}{||\Vec{u}||^2}\vec{u}$\\
        $\bullet proj_w \vec{b} = P\Vec{b}$, $w$ = col$(A)$\\
        $\bullet P_{\text{Row}(A)} = I - P_{\text{Col}(A)}$\\
        $\bullet \vec{v} \in \text{Nul}(A) \Rightarrow \forall \vec{u} \text{ s.t. } \vec{u} \cdot \vec{v} = 0, \vec{u} \in \text{Row}(A)$\\
        $\bullet \vec{v} \in w \Leftrightarrow proj_w \vec{v} = \vec{v}$, so $P\Vec{v} = \vec{v} \Rightarrow v \in w$\\
        $\bullet \vec{v} \in w^\perp \Leftrightarrow proj_w \vec{v} = 0$, so $P\Vec{v} = 0 \Rightarrow v \in w^\perp$\\
        $\bullet$ Rank$(P_\text{subspace})$ = dim(subspace)\\
        $\bullet ||P_{\text{Row}(A)} \vec{x}||$ is shortest distance from $\vec{x}$ to Nul$(A)$\\
        $\bullet P = A(A^T A)^{-1}A^T$ In general (subspace)\\
        $\bullet P = \frac{(\vec{a})(\vec{a}^T)}{(\vec{a}^T)  (\vec{a})}$ (for line only)\\
        \textbf{Shortcut for Orthonormal vectors:}\\
        $P\vec{b} = (\vec{a_1} \cdot \vec{b})\Vec{a_1} + 
        (\vec{a_2} \cdot \vec{b})\Vec{a_2} + ... + (\vec{a_n} \cdot \vec{b})\Vec{a_n}$\\
        \textbf{Shortcut for Orthogonal vectors:}\\
        $P\vec{b} = proj_{\vec{a_1}} b + 
        proj_{\vec{a_2}} b + ... + proj_{\vec{a_n}} b$\\
        \textbf{Example:} $P_W \begin{bmatrix}
            1\\1\\1
        \end{bmatrix} = \begin{bmatrix}
            0\\-2\\-2
        \end{bmatrix}$, find $W$\\
        $\vec{b} = \begin{bmatrix}
            1 & 1 & 1
        \end{bmatrix}^T \notin W, \vec{b} \notin W^\perp$\\
        so $\vec{z} = \vec{b} - P\vec{b}$, and $\vec{z} \in W^\perp$\\
        $\vec{z} = \begin{bmatrix}
            1\\1\\1
        \end{bmatrix} - \begin{bmatrix}
            0\\-2\\-2
        \end{bmatrix} = \begin{bmatrix}
            1\\3\\3
        \end{bmatrix} \Rightarrow W$ given by $x + 3y + 3z = 0$
    \end{minipage}
};
%------------ Projections Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Projections};
\end{tikzpicture}


%------------ Singular Value Decomposition Content---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $A = U \Sigma V^T$; orthogonal, diagonal, orthogonal\\
        \textbf{Finding an SVD: $A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n}$}\\
        1. orthogonally diagonalize $A^TA$\\
        2. $\Sigma$: diagonal decreasing $\sqrt{\text{eigenvalues}}$, 0's elsewhere\\
        3. $V$: $\vec{\text{eigen}}$'s corresponding to $\Sigma$'s $\sqrt{\text{eigenvalues}}$\\
        4a. $\vec{u_i} = \frac{1}{\sigma_i}A \vec{v_i}$, while possible\\
        4b. $\vec{u}_{(n+1...m)}$ = Nul$\left( \begin{bmatrix}
            \vec{u_1}^T\\ \vdots \\ \vec{u_n}^T
        \end{bmatrix} \right)$ ($\because U$ is orthogonal)
        \textbf{Pieces of SVD:} $A = \sum_{i=1}^r = \sigma_i \vec{u}_i \vec{v}_i^T$\\
        $\bullet \sum_{i=1}^\phi = \sigma_i \vec{u}_i \vec{v}_i^T$ is best rank $\phi$ approximation of $A$
        \textbf{Reduced SVD: $A = U_r \Sigma_r V_r ^T$}\\
            \indent\hspace{0.5cm}$\bullet A_{m \times n} = U_{m \times r} \Sigma_{r \times r} V_{r \times n} ^T$\\
        \textbf{Psuedo-Inverse $A^+ = V_r \Sigma_r^{-1} U_r^T$:}\\
            \indent\hspace{0.5cm}$\bullet \hat{x} = A^+ \vec{b}$\\
            \indent\hspace{0.5cm}$\bullet A A^+ = P_{\text{Col}(A)} = U_r U_r^T$\\
            \indent\hspace{0.5cm}$\bullet A^+ A = P_{\text{Row}(A)} = V_r V_r^T$\\
        \textbf{Geometry of SVD:}\\
            \indent\hspace{0.5cm}1. $\sigma_1$: max length of ellipsoid\\
            \indent\hspace{0.5cm}2. $\vec{v_1}:$ $A \vec{v}$ maximizes len(ellipsoid)\\
            \indent\hspace{0.5cm}3. $\vec{u_1}$: direction of largest axis\\
        $\bullet$ rotate, stretch, rotate (like orthog. diagonalization) \\
        $\bullet A$Row$(A)$ = Col($A$) = Row($A^T$)\\
        $\bullet$ $||\vec{x}|| = 1 \Rightarrow$ max $(||A\vec{x}||) = \sigma_1$. Occurs at $\vec{x}$ = corresponding $\vec{\text{eigen}}$.\\
        \textbf{Polar Decomposition: $A = QS$}: (rotate, stretch)\\
            1. $S = V \Sigma V^T$ is symmetric positive semidefinite\\
            2. $Q = U V^T$ is orthogonal\\
        \textbf{Bases of 4 fundamental subspaces from SVD:}\\
            \indent\hspace{0.25cm}1. Col($A$): first $r$ cols of $U$\\
            \indent\hspace{0.25cm}2. Nul($A^T$): cols beyond $r$th of $U; (r+1 ... m$)\\
            \indent\hspace{0.25cm}3. Row($A$): first $r$ rows of $V^T$\\
            \indent\hspace{0.25cm}4. Nul($A$): rows beyond $r$th of $V^T; (r+1 ... n)$
    \end{minipage}
};
%------------ Singular Value Decomposition Header---------------------
\node[fancytitle, right=10pt] at (box.north west) {Singular Value Decomposition};
\end{tikzpicture}

%------------ Applications of SVD to Stats Content---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $B_{m \times n}$: $A$ w/ elements as value - row's mean\\
        \textbf{Covariance Matrix: }$S_{m \times m} = \frac{1}{n-1}B B^T$\\
        \textbf{PCA:} Principal Components are unit $\vec{\text{eigen}}$ of S\\
        \indent\hspace{0.5cm}$\bullet$ Total Variance = $\sum_{i=1}^m S_{(i,i)} = \sum_{i=1}^m \lambda_i$\\
        \indent\hspace{0.5cm}$\bullet$ var(principle component $i$) $= \lambda_i$
        % $\bullet$ Total Variance = $\sum_{i=1}^n \lambda_i$
    \end{minipage}
};
%------------ Applications of SVD to Stats Header---------------------
\node[fancytitle, right=10pt] at (box.north west) {Applications of SVD to Stats};
\end{tikzpicture}


%------------ Least Squares Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $\hat{x} = (A^T A)^{-1}A^T \Vec{b}$\\
        - \textbf{Normal equations} for $A\Vec{x} = \Vec{b}$: $A^TA\Vec{x} = A^T \Vec{b}$\\
        $\bullet A^TA\Vec{x} = A^T \Vec{b}$ has same solutions as $\hat{x}$.\\
        $\bullet A \hat{x} = proj_w \Vec{b}$\\
        $\bullet \hat{x} = R^{-1}Q^T \Vec{b}$, where $A = QR$ factorization\\
        $\bullet \hat{x} = \vec{x}$ s.t. $A \hat{x}$ is as close as possible to $\vec{b}$\\
        $\bullet \hat{x} = A^+ \vec{b}$
    \end{minipage}
};
%------------ Least Squares Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Least Squares};
\end{tikzpicture}


%------------ Orthogonal Diagonalization & Spectral Decomposition Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        - $A = QDQ^T$ for orthogonal $Q$\\
        - $A^T = A \Rightarrow A \vec{x} = (A \vec{x})^T$\\
        \textbf{Spectal Theorem for Orthognal Matrices:} \\
        \indent\hspace{0.5cm}1. $A$ is orthogonally diagonalizable iff $A=A^T$\\
        \indent\hspace{0.5cm}2. The dimension of the eigenspace for each eigenvalue $\lambda$ equals the multiplicity of $\lambda$ as a root of the characteristic equation\\
        \indent\hspace{0.5cm}3. $\Vec{Eigen}$ for different $\lambda$'s orthogonal\\
        \indent\hspace{0.5cm}4. Symmetric $A$ has real eigenvalues\\
        \textbf{Spectral Decomposition:}\\
        $A = \sum_{i=1}^n \lambda_i \vec{u_i} \Vec{u_i}^T \Leftrightarrow (Q = [\Vec{u_1}, \Vec{u_2}, ..., \Vec{u_n}]$)
        
    \end{minipage}
};
%------------ Orthogonal Diagonalization & Spectral Decomposition Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Diagonalization \& Spec. Decomp.};
\end{tikzpicture}


%------------ Quadratic Forms Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $A_{n \times n}$ has quadratic form $Q(\vec{x}) = \vec{x}^T A \Vec{x}$ for $\Vec{x} \in \mathbb{R}^n$\\
        $Q(x) = a_1 x_1^2 + a_2 x_1 x_2 + a_3 x_2^2 \Leftrightarrow A = \begin{bmatrix}
            a_1 & \frac{a_2}{2}\\
            \frac{a_2}{2} & a_2
        \end{bmatrix}$\\
        \textbf{- Definite:} $Q(\vec{x}) < 0 \forall x \neq 0 \Leftrightarrow \lambda < 0 \forall \lambda$\\
        \textbf{- Semidefinite:} $Q(\vec{x}) \leq 0 \forall x \neq 0 \Leftrightarrow \lambda \leq 0 \forall \lambda$\\
        \textbf{+ Definite:} $Q(\vec{x}) > 0 \forall x \neq 0 \Leftrightarrow \lambda > 0 \forall \lambda$\\
        \textbf{+ Semidefinite:} $Q(\vec{x}) \geq 0 \forall x \neq 0 \Leftrightarrow \lambda \geq 0 \forall \lambda$\\
        $\bullet$ Semidefinite has $\infty$ mins/maxes\\
        $\bullet$ Definite matrices are symmetric\\
        $\bullet$ $Q(x)$ of $A_{n \times n}$ has $n$ $x_i^2$ terms\\
        $\bullet$ $||\vec{x}||=1 \Rightarrow $min$Q(\vec{x}) = $ min($\lambda$), max$Q(\vec{x}) = $ max($\lambda$)\\
        $\bullet$ Max occurs at $\vec{x} = \frac{1}{||\vec{\text{eigen}}||}\vec{\text{eigen}}$ for max($\lambda$)\\
        \textbf{Principal Axes Theorem:}\\
        Symmetric $A$ has orthogonal change of vars $\vec{x} = P\vec{y}$, transforms $\vec{x}^T A \vec{x}$ into $\vec{y}^T Q \vec{y}$ w/ no cross-prod term.\\
        - After change of vars, coeff's on $y_i^2$ are eigenvalues
    \end{minipage}
};
%------------ Quadratic Forms Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Quadratic Forms of Symmetric Matrices};
\end{tikzpicture}


\end{multicols*}
\end{document}

