
\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\makeatletter

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\title{MATH 2LA3 (Applications of Linear Algebra): Midterm 2 Summary Sheet}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\advance\paperheight1.5in % here is new https://tex.stackexchange.com/questions/649149/cheat-sheet-using-mini-pages-and-tikzpicture-does-not-overflow-properly

\makeatletter  % for augmanted matrix from https://tex.stackexchange.com/questions/2233/whats-the-best-way-make-an-augmented-coefficient-matrix
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% these two packages needed for dotted vertical line in augmented matrix
\usepackage{amsmath}
\usepackage{arydshln} 

\usepackage{multirow} % for multi column

\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

\begin{center}{\huge{\textbf{MATH 2LA3: Midterm 2 Summary Sheet}}}\\
\end{center}
\begin{multicols*}{3}

\tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

%------------ Orthogonal Matrices ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $Q_{n \times n}$ is an orthogonal matrix iff:\\
        1. Columns of $Q$ form an orthogonal set (all orthogonal to each other)\\
        2. Magnitude of each column = 1\\
    Properties:
    
        1. $(Q\vec{x})^T(Q\Vec{z}) = \vec{x}^Tz$ (angle preserving)\\
        2. $||Q\vec{x}||=||\vec{x}||$ (length preserving)\\
        3. $Q^T = Q^{-1}$ (true iff $Q$ is orthogonal)\\
        4. $Q$ is invertible\\
        5. $\det(Q) = \pm 1$\\
        6. $Q^TQ = I_n$\\
    $\det(Q) = 1$ for rotation $\det(Q) = -1$ for reflection
    \end{minipage}
};
%------------ Orthogonal Matrices Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Matrices};
\end{tikzpicture}

%------------ Markov Chains ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    Let $A$ be a regular transition matrix: $A = 
    \begin{bmatrix}
        0.1 & 0.6\\
        0.9 & 0.4
    \end{bmatrix}$
    To find the long term probability the system will be at a particular state, find the eigenvector corresponding to $\lambda = 1$ and scale it to a probability vector $\vec{p}$.\\
    
    $[A | 0] \sim
    \begin{bmatrix}[cc:c]
      1 & \frac{-2}{3} & 0\\
      0 & 0 & 0
    \end{bmatrix}$\\
    
    $\Rightarrow \vec{x} = x_2
    \begin{bmatrix}
        \frac{2}{3}\\
        1
    \end{bmatrix}$\\
    $\Rightarrow \vec{p} = \frac{1}{\frac{2}{3} + 1}
    \begin{bmatrix}
        \frac{2}{3}\\
        1
    \end{bmatrix} = 
    \begin{bmatrix}
        0.4\\
        0.6
    \end{bmatrix}$
    $
    \begin{matrix}
        \leftarrow \text{Long term $P$(in state 1)}\\
        \leftarrow \text{Long term $P$(in state 2)}
    \end{matrix}$
    \end{minipage}
};
%------------ Markov Chains Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Markov Chains};
\end{tikzpicture}

%------------ Diagonalization Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $D = 
    \begin{bmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \ddots & 0 \\
        0 & 0 & \lambda_n
    \end{bmatrix}$\\
    $P = \begin{bmatrix}
        \vec{\text{eigen}_{\lambda_1}} & \hdots & \vec{\text{eigen}_{\lambda_n}}
    \end{bmatrix}$\\
    $A$ and $D$ are similar and $A$ is diagonalizable iff $A = PDP^{-1}$.\\
    - $A$ is similar to itself.\\
    $A$ is diagonalizable iff (equivalent statements):
    \begin{itemize}
        \item $A$ has $n$ linearly independent eigenvectors
        \item $\sum_{i} \text{GM}(\lambda_i) = n$. GM($\lambda$) := number of eigenvectors for eigenvalue $\lambda$
    \end{itemize}
    $A$ diagonalizable $\Leftrightarrow A^n = PD^nP^{-1}$\\
    
    
    \end{minipage}
};
%------------ Diagonalization Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Diagonalization};
\end{tikzpicture}

%------------ Orthogonal Compliment ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    Definition: if $W$ is a subspace of $\mathbb{R}^n$, then $W^{\perp}$ contains all vectors $\in \mathbb{R}^n$ perpendicular to $W$\\
    Properties:
    \begin{enumerate}
        \item $W^{\perp} =$ Null($A^T$)
        \item dim($W$) + dim($W^{\perp}) = n$
        \item $(W^{\perp})^{\perp} = W$
        \item $W \cap W{^\perp} = \{ \vec{0} \}$
        \item $(\mathbb{R}^n)^{\perp} = \{ \vec{0} \}$; $\{ \vec{0} \}^{\perp} = \mathbb{R}^n$
    \end{enumerate}
    Subspaces of $\mathbb{R}^n$:
    \begin{itemize}
        \item Row($A$) = (Nul($A$))$^{\perp}$
        \item Nul($A$) = (Row($A$))$^{\perp}$
        \item dim(Row($A$)) = Rank($A$)
        \item dim(Nul($A$)) = $n$ - Rank($A$)
    \end{itemize}
    Subspaces of $\mathbb{R}^m$:
    \begin{itemize}
        \item Col($A$) = (Nul($A^T$))$^{\perp}$
        \item Nul($A^T$) = (Col($A$))$^{\perp}$
        \item dim(Col($A$)) = Rank($A$)
        \item dim(Nul($A^T$)) = $m$ - Rank($A$)
    \end{itemize}
    \textbf{Example: find a basis for Row($A$)}\\
    $\vec{v} = \begin{bmatrix} 1\\ 2\\ 1\\ 2 \end{bmatrix}$, and $A\vec{v} = \vec{0}$\\
    $A\vec{v} = \vec{0}$, so $\vec{v} \in$ Nul($A$).\\
    Let $W = $Nul($A$), so $W^\perp =$ Row($A$)\\
    $\Rightarrow $ Nul($\Vec{v}^T$) is a basis for Row($A$).\\
    \begin{align*}
        \text{Nul}(\begin{bmatrix} 1&2&1&2 \end{bmatrix}) &= \left\{ \begin{bmatrix} -2\\ 1\\ 0\\ 0 \end{bmatrix}, \begin{bmatrix} -1\\ 0\\ 1\\ 0 \end{bmatrix}, \begin{bmatrix} -2\\ 0\\ 0\\ 1 \end{bmatrix} \right\} \\
        &= \text{basis for Row}(A)
    \end{align*}
        
    \end{minipage}
};
%------------ Orthogonal Compliment Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Compliment};
\end{tikzpicture}




%------------ Matrix Properties Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    $A_{m \times n} \Rightarrow $ transformation $\vec{x} \rightarrow A\vec{x}$ is from $\mathbb{R}^n \rightarrow \mathbb{R}^m$\\
    Rank$(A)$ + nullity($A$) = $n$
        \hrule
    
    \small{
    	\begin{tabular}{lp{4cm} l}
		\textbf{Determinants} & $\det(AB) = \det(A)\det(B)$ \\
                & $\det(A^{-1}) = \frac{1}{\det(A)}$ \\
                & $\det(\text{adj}(A)) = \det(A)^{n-1}$ \\
                & $\det(A^n) = ({\det(A)})^n$ \\
                & $\det(cA) = c^n\det(A)$ \\ 
                & $\det(A^T) = \det(A)$ \\ 
                & $\det(A_{\text{tringl}}) = \prod_{i=1}^n A_{(i,i)}$ \\ 
                & $\det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc$\\
            \hline

		\textbf{Inverse} & $(AB)^{-1} = B^{-1}A^{-1}$ \\
                & $(A^{-1})^{-1} = A$ \\
                & $(kA^{-1}) = \frac{1}{k}A$ \\
                & $(A^{-1})^{-1} = I_n$ \\ 
                
                & \multirow{2}{*}{$\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$} \\
                & \\
                & \\
            \hline
            
		\textbf{Transpose} & $(A^T)^T = A$ \\
                & $(AB)^{T} = B^{T}A^{T}$ \\
                & $(kA^{T}) = kA^T$ \\
                & $(A + B)^T = A^T + B^T$ \\ 
            % \hline
	\end{tabular}}
    \end{minipage}
};
%------------ Matrix Properties Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Matrix Properties};
\end{tikzpicture}

%------------ Eigenvalues and Eigenvectors Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    The Eigenvalues of a triangular or diagonal matrix are along the main diagonal.\\
    - AM($\lambda = 0$) = nullity($A$)\\
    To \textbf{Find Eigenvalues} of $A = 
    \begin{bmatrix}
        -5 & 2 \\
        -9 & 6
    \end{bmatrix}$\\
    Solve $ |A-\lambda I_n| = 0 \Leftrightarrow
        \begin{bmatrix}
            -5 - \lambda & 2 \\
            -9 & 6 - \lambda
        \end{bmatrix} = 0$

    To \textbf{Find Eigenvectors for $\lambda_i$}, solve:\\
        $\begin{bmatrix}[cc:c]
            -5 - \lambda_i & 2 & 0 \\
            -9 & 6 - \lambda_i & 0
        \end{bmatrix}$
    \end{minipage}
};
%------------ Eigenvalues and Eigenvectors Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Eigenvalues and Eigenvectors};
\end{tikzpicture}


%------------ Dot Product Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    -  $\begin{bmatrix}
        u_1 \\
        u_2
    \end{bmatrix} \cdot
    \begin{bmatrix}
        v_1\\
        v_2
    \end{bmatrix} = (u_1)(v_1) + (u_2)(v_2)
    $\\
    - $\vec{u} \cdot \vec{v} = \vec{u}^T \vec{v}$\\
    - $\vec{u} \cdot \Vec{u} = \left( ||\vec{u}|| \right)^2$\\
    - $\vec{u} \cdot \vec{v} = 0 \Leftrightarrow \vec{u}$, $\vec{v}$ orthogonal $\Leftrightarrow ||\vec{u}|| + ||\vec{v}|| = ||\vec{u} + \vec{v}||$\\
    - $(\vec{a} + \vec{b}) \cdot (\vec{c} + \Vec{d}) = \vec{a} \cdot \vec{c} + \vec{a} \cdot \vec{d} + \vec{b} \cdot \vec{c} + \vec{b} \cdot \vec{d}$
    - $||\begin{bmatrix} v_1 \\ v_2 \end{bmatrix}|| = \sqrt{v_1^2 + v_2^2}$
    \end{minipage}
};
%------------ Dot Product Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Dot Product};
\end{tikzpicture}

%------------ Span Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
    \textbf{Def} The span of a set of vectors is the set of linear combinations of the vectors.\\
    Check if $\vec{w} \in $span$\{\vec{u}, \vec{v} \}$:\\
    $\begin{bmatrix}[cc:c]
            u_1 & v_1 & w_1 \\
            u_2 & v_2 & w_2
        \end{bmatrix}$ $\vec{w} \in $span$\{\vec{u}, \vec{v} \}$ iff that system has a solution.

    Check if span$\{\vec{u}, \vec{v}, \vec{w}\} = \mathbb{R}^3:$\\ (equivalent to col[$\Vec{u}, \vec{v}, \vec{w}$] = $\mathbb{R}^3$)\\
    $\begin{bmatrix}
            u_1 & v_1 & w_1 \\
            u_2 & v_2 & w_2 \\
            u_3 & v_3 & w_3
        \end{bmatrix}$ True iff that matrix has a pivot in each row/column
    \end{minipage}
};
%------------ Span Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Span};
\end{tikzpicture}


%------------ Injectivity and Surjectivity Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \textbf{Injective:} (one-to-one)\\
        1. $f(x) = f(y) \leftarrow x = y$ (map to the same point)\\
        2. rank($A$) = n\\
        3. pivot in every column

        \textbf{Surjective:} (onto)\\
        1. $\forall \vec{b} \in \mathbb{R}^m, \exists \vec{x} \in \mathbb{R}^n$ such that $f(\vec{x}) = \vec{b}$\\
        2. rank($A$) = m\\
        3. pivot in every row 
    \end{minipage}
};
%------------ Injectivity and Surjectivity Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Injectivity and Surjectivity};
\end{tikzpicture}

%------------ Invertible Matrix Theorem Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        
        1. A is row-equivalent to the \(n \times n\) \(I_n\). \\
        2. A has \(n\) pivot positions. \\
        3. The equation \(Ax=0\) has only the trivial solution \(x=0\). \\
        4. The columns of A form a linearly independent set. \\
        5. The linear transformation \(x \mapsto Ax\) is one-to-one. \\
        6. For each column vector \(b\) in \(\mathbb{R}^n\), the equation \(Ax=b\) has a unique solution. \\
        7. The columns of A span \(\mathbb{R}^n\). \\
        8. The linear transformation \(x \mapsto Ax\) is a surjection. \\
        9. There is an \(n \times n\) matrix \(C\) such that \(CA=I_n\). \\
        10. There is an \(n \times n\) matrix \(D\) such that \(AD=I_n\). \\
        11. The transpose matrix \(A^T\) is invertible. \\
        12. The columns of A form a basis for \(\mathbb{R}^n\). \\
        13. The column space of A is equal to \(\mathbb{R}^n\). \\
        14. The dimension of the column space of A is \(n\). \\
        15. The rank of A is \(n\). \\
        16. The null space of A is \(\{0\}\). \\
        17. The dimension of the null space of A is 0. \\
        18. 0 fails to be an eigenvalue of A. \\
        19. The determinant of A is not zero. \\
        20. The orthogonal complement of the column space of A is \(\{0\}\). \\
        21. The orthogonal complement of the null space of A is \(\mathbb{R}^n\). \\
        22. The row space of A is \(\mathbb{R}^n\).
        
    \end{minipage}
};
%------------ Invertible Matrix Theorem Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Invertible Matrix Theorem};
\end{tikzpicture}


%------------ Least Squares Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $\hat{x} = (A^T A)^{-1}A^T \Vec{b}$\\
        - \textbf{Normal equations} for $A\Vec{x} = \Vec{b}$: $A^TA\Vec{x} = A^T \Vec{b}$\\
        - $A^TA\Vec{x} = A^T \Vec{b}$ has same solutions as $\hat{x}$.\\
        - $A \hat{x} = proj_w \Vec{b}$\\
        - $\hat{x} = R^{-1}Q^T \Vec{b}$, where $A = QR$ factorization\\
        - $\hat{x} = \vec{x}$ s.t. $A \hat{x}$ is as close as possible to $\vec{b}$
    \end{minipage}
};
%------------ Least Squares Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Least Squares};
\end{tikzpicture}


%------------ Projections Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        - Projection matrices are symmetric.\\
        - $proj_{\vec{u}}\Vec{v} = \frac{\vec{v} \cdot \vec{u}}{||\Vec{u}||^2}\vec{u}$\\
        - $proj_w \vec{b} = P\Vec{b}$, $w$ = col$(A)$\\
        - $P_{\text{Row}(A)} = I - P_{\text{Col}(A)}$\\
        - $\vec{v} \in \text{Nul}(A) \Rightarrow \forall \vec{u} \text{ s.t. } \vec{u} \cdot \vec{v} = 0, \vec{u} \in \text{Row}(A)$\\
        - $\vec{v} \in w \Leftrightarrow proj_w \vec{v} = \vec{v}$, so $P\Vec{v} = \vec{v} \Rightarrow v \in $Nul($A$)\\
        - $\vec{v} \in w^\perp \Leftrightarrow proj_w \vec{v} = 0$, so $P\Vec{v} = 0 \Rightarrow v \in $Col($A$)\\
        - Rank$(P_\text{subspace})$ = dim(subspace)\\
        - $||P_{\text{Row}(A)} \vec{x}||$ is shortest distance from $\vec{x}$ to Nul$(A)$\\
        - $P = A(A^T A)^{-1}A^T$ In general (subspace)\\
        - $P = \frac{(\vec{a})(\vec{a}^T)}{(\vec{a}^T)  (\vec{a})}$ (for line only)\\
        \textbf{Shortcut for Orthonormal vectors:}\\
        $P\vec{b} = (\vec{a_1} \cdot \vec{b})\Vec{a_1} + 
        (\vec{a_2} \cdot \vec{b})\Vec{a_2} + ... + (\vec{a_n} \cdot \vec{b})\Vec{a_n}$\\
        \textbf{Shortcut for Orthogonal vectors:}\\
        $P\vec{b} = proj_{\vec{a_1}} b + 
        proj_{\vec{a_2}} b + ... + proj_{\vec{a_n}} b$\\
        \textbf{Example:} $P_W \begin{bmatrix}
            1\\1\\1
        \end{bmatrix} = \begin{bmatrix}
            0\\-2\\-2
        \end{bmatrix}$, find $W$\\
        $\vec{b} = \begin{bmatrix}
            1\\1\\1
        \end{bmatrix} \notin W, \vec{b} \notin W^\perp$\\
        so $\vec{z} = \vec{b} - P\vec{b}$, and $\vec{z} \in W^\perp$\\
        $\vec{z} = \begin{bmatrix}
            1\\1\\1
        \end{bmatrix} - \begin{bmatrix}
            0\\-2\\-2
        \end{bmatrix} = \begin{bmatrix}
            1\\3\\3
        \end{bmatrix} \Rightarrow W$ given by $x + 3y + 3z = 0$
    \end{minipage}
};
%------------ Projections Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Projections};
\end{tikzpicture}

%------------ Gram-Schmidt Process Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        \textbf{Gram-Schmidt Process:} (orthogonalize basis)\\
        $\vec{v_1} = \vec{x_1}$ (remember to normalize at end)\\
        $\vec{v_2} = \vec{x_2} - \frac{\vec{x_2} \cdot \vec{v_1}}{\vec{v_1} \cdot \vec{v_1}}\vec{v_1}$\\
        $\vec{v_p} = \vec{x_p} - \frac{\vec{x_p} \cdot \vec{v_1}}{\vec{v_1} \cdot \vec{v_1}}\vec{v_1} - \frac{\vec{x_p} \cdot \vec{v_2}}{\vec{v_2} \cdot \vec{v_2}}\vec{v_2}- ... \frac{\vec{x_p} \cdot \vec{v_{p-1}}}{\vec{v_{p-1}} \cdot \vec{v_{p-1}}}\vec{v_{p-1}}$\\
        \textbf{QR Factorization ($A = QR$):}\\
        $A_{n\times q}$ has independent columns.  Apply Gram-Schmidt on columns to get $\{\vec{v_1},..,\vec{v_q} \}$, then normalize to $\{\vec{u_1},..,\vec{u_q} \}$\\
        - $Q = [\vec{u_1},..,\vec{u_q}]$\\
        - $R = Q^T A$\\
        - $R_{(i, j)} = u_i \cdot a_j$ ($R_{q \times q}$ will be upper triangular)
    \end{minipage}
};
%------------ Gram-Schmidt Process Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Gram-Schmidt Process};
\end{tikzpicture}


%------------ Orthogonal Diagonalization & Spectral Decomposition Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        - $A = QDQ^T$ for orthogonal $Q$\\
        - $A$ is orthogonally diagonalizable iff $A=A^T$\\
        - Symmetric $A$ has $n$ real eigenvalues, w/ multiplicity\\
        - $\Vec{Eigen}$ for different $\lambda$ orthogonal\\
        - $A^T = A \Rightarrow A \vec{x} = (A \vec{x})^T$\\
        \textbf{Spectral Decomposition:}\\
        $A = \sum_{i=1}^n \lambda_i \vec{u_i} \Vec{u_i}^T \Leftrightarrow (Q = [\Vec{u_1}, \Vec{u_2}, ..., \Vec{u_n}]$)
        
    \end{minipage}
};
%------------ Orthogonal Diagonalization & Spectral Decomposition Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Diagonalization \& Spec. Decomp.};
\end{tikzpicture}


%------------ Quadratic Forms Content ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $A_{n \times n}$ has quadratic form $Q(\vec{x}) = \vec{x}^T A \Vec{x}$ for $\Vec{x} \in \mathbb{R}^n$\\
        $Q(x) = a_1 x_1^2 + a_2 x_1 x_2 + a_3 x_2^2 \Leftrightarrow A = \begin{bmatrix}
            a_1 & \frac{a_2}{2}\\
            \frac{a_2}{2} & a_2
        \end{bmatrix}$\\
        \textbf{- Definite:} $Q(x) < 0 \forall x \neq 0 \Leftrightarrow \lambda < 0 \forall \lambda$\\
        \textbf{- Semidefinite:} $Q(x) \leq 0 \forall x \neq 0 \Leftrightarrow \lambda \leq 0 \forall \lambda$\\
        \textbf{+ Definite:} $Q(x) > 0 \forall x \neq 0 \Leftrightarrow \lambda > 0 \forall \lambda$\\
        \textbf{+ Semidefinite:} $Q(x) \geq 0 \forall x \neq 0 \Leftrightarrow \lambda \geq 0 \forall \lambda$\\
        - Definite matrices are symmetric\\
        - $Q(x)$ of $A_{n \times n}$ has $n$ $x_i^2$ terms\\
        - $||\vec{x}||=1 \Rightarrow $min$Q(\vec{x}) = $ min($\lambda$), max$Q(\vec{x}) = $ max($\lambda$)\\
        - Max occurs at $\vec{x} = \frac{1}{||\vec{\text{eigen}}||}\vec{\text{eigen}}$ for max($\lambda$)\\
        \textbf{Principal Axes Theorem:}\\
        Symmetric $A$ has orthogonal change of vars $\vec{x} = P\vec{y}$, transforms $\vec{x}^T A \vec{x}$ into $\vec{y}^T Q \vec{y}$ w/ no cross-prod term.\\
        - After change of vars, coeff's on $y_i^2$ are eigenvalues
    \end{minipage}
};
%------------ Quadratic Forms Header ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Quadratic Forms of Symmetric Matrices};
\end{tikzpicture}


%------------ Orthogonal Decomposition ---------------
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.3\textwidth}
        $W$ a subspace of $\mathbb{R}^n \Leftrightarrow y = \hat{y} + z$, s.t. $\hat{y} \in W, z \in W^\perp$\\
        $\hat{y} = proj_{u_1}y +...+ proj_{u_p}y = proj_{W}y = \sum_{i=1}^p \frac{y \cdot u_i}{||u_i||^2}$
    \end{minipage}
};
%------------ Orthogonal Decomposition ---------------------
\node[fancytitle, right=10pt] at (box.north west) {Orthogonal Decomposition};
\end{tikzpicture}



\end{multicols*}
\end{document}

